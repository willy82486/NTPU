{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2-1 Phoneme Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --no-cache-dir gdown\n",
    "# !gdown --id '1iDmtJ8vg-SF8dC0r0AoOg4n5UdVOj9To' --output data.zip\n",
    "# !unzip data.zip\n",
    "# !ls "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For data preprocess\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Utility\n",
    "import gc\n",
    "\n",
    "my_seed = 0\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(my_seed)\n",
    "torch.manual_seed(my_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(my_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Size of training data: (922449, 429)\n",
      "Size of testing data: (307483, 429)\n"
     ]
    }
   ],
   "source": [
    "print (\"Loading data ...\")\n",
    "\n",
    "data_root = \"./timit_11/\"\n",
    "train_data = np.load(data_root + \"train_11.npy\")\n",
    "train_label = np.load(data_root + \"train_label_11.npy\")\n",
    "test_data = np.load(data_root + \"test_11.npy\")\n",
    "\n",
    "print(\"Size of training data: {}\".format(train_data.shape))\n",
    "print(\"Size of testing data: {}\".format(test_data.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number 922449\n",
      "\n",
      "   class   count    rate\n",
      "       0   47070  0.0510\n",
      "       1   62434  0.0677\n",
      "       2   26807  0.0291\n",
      "       3   44575  0.0483\n",
      "       4   29669  0.0322\n",
      "       5   20282  0.0220\n",
      "       6    2952  0.0032\n",
      "       7   55166  0.0598\n",
      "       8   21464  0.0233\n",
      "       9   25429  0.0276\n",
      "      10    8029  0.0087\n",
      "      11    8789  0.0095\n",
      "      12   20028  0.0217\n",
      "      13   32881  0.0356\n",
      "      14   29662  0.0322\n",
      "      15    8600  0.0093\n",
      "      16   15788  0.0171\n",
      "      17   38796  0.0421\n",
      "      18   18675  0.0202\n",
      "      19   35149  0.0381\n",
      "      20    6427  0.0070\n",
      "      21    5415  0.0059\n",
      "      22    5404  0.0059\n",
      "      23    7953  0.0086\n",
      "      24    2811  0.0030\n",
      "      25    6099  0.0066\n",
      "      26    5689  0.0062\n",
      "      27    4523  0.0049\n",
      "      28    8531  0.0092\n",
      "      29   15552  0.0169\n",
      "      30   18653  0.0202\n",
      "      31   23591  0.0256\n",
      "      32    9047  0.0098\n",
      "      33   16663  0.0181\n",
      "      34    5098  0.0055\n",
      "      35   63362  0.0687\n",
      "      36   20142  0.0218\n",
      "      37   10604  0.0115\n",
      "      38  134640  0.1460\n"
     ]
    }
   ],
   "source": [
    "# 統計每個類別的數量\n",
    "print (\"Total number {:d}\".format(train_label.shape[0]))\n",
    "\n",
    "train_cnt = np.zeros((39), dtype=int)\n",
    "for i in range(39):\n",
    "    train_cnt[i] = np.sum(train_label == str(i))\n",
    "\n",
    "sum = np.sum(train_cnt)\n",
    "print (\"\\n   class   count    rate\")\n",
    "for i in range(39):\n",
    "    print (\"{:8d}\".format(i), end='')\n",
    "    print (\"{:8d}\".format(train_cnt[i]), end='')\n",
    "    print (\"  {:.4f}\".format(train_cnt[i] / sum))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TIMITDataset(Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        if y is not None:\n",
    "            # y = y.astype(np.int)\n",
    "            y = y.astype(int)\n",
    "            self.y = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.y is not None:\n",
    "            return self.x[index], self.y[index]\n",
    "        else:\n",
    "            return self.x[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超參數\n",
    "valid_rate = 0.2          \n",
    "num_epoch = 200             \n",
    "learning_rate = 0.0001      \n",
    "weight_decay_l1 = 0.0\n",
    "weight_decay_l2 = 0.001\n",
    "batch_size = 2048           # 原本設64，跑起來更慢，且沒辦法代表分佈\n",
    "number = 10000              # 每個類別最少補到一萬筆data\n",
    "\n",
    "model_path = \"./model.ckpt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打亂分佈，隨機選取training data跟valid data\n",
    "train_indices, valid_indices = train_test_split([i for i in range(train_data.shape[0])], test_size=valid_rate, random_state=1)\n",
    "train_x = train_data[train_indices, :]\n",
    "train_y = train_label[train_indices]\n",
    "valid_x = train_data[valid_indices, :]\n",
    "valid_y = train_label[valid_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 紀錄每個類的id，方便sample\n",
    "train_class = []\n",
    "id = np.arange(train_x.shape[0])\n",
    "for i in range(39):\n",
    "    train_class.append(id[train_y == str(i)])\n",
    "\n",
    "del train_data, train_label\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(429, 2048)\n",
    "        self.layer2 = nn.Linear(2048, 2048)\n",
    "        self.layer3 = nn.Linear(2048, 2048)\n",
    "        self.layer4 = nn.Linear(2048, 1024)\n",
    "        self.layer5 = nn.Linear(1024, 512)\n",
    "        self.layer6 = nn.Linear(512, 128)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(2048)\n",
    "        self.bn2 = nn.BatchNorm1d(2048)\n",
    "        self.bn3 = nn.BatchNorm1d(2048)\n",
    "        self.bn4 = nn.BatchNorm1d(1024)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.bn6 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.out = nn.Linear(128, 39) \n",
    "        \n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.act_fn = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.layer5(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.layer6(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check device\n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_regularization(model, weight_decay_l1, weight_decay_l2):\n",
    "    l1 = 0\n",
    "    l2 = 0\n",
    "    for i in model.parameters():\n",
    "        l1 += torch.sum(abs(i))\n",
    "        l2 += torch.sum(torch.pow(i, 2))\n",
    "    return weight_decay_l1 * l1 + weight_decay_l2 * l2\n",
    "\n",
    "def train_model(num_epoch, learning_rate, weight_decay_l1, weight_decay_l2,\n",
    "                train_dataset, train_dataloader,\n",
    "                valid_dataset, valid_dataloader):\n",
    "    model = Classifier().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epoch):\n",
    "        # 前面使用adam，收斂快，後面使用SGDM，穩定且偏差小\n",
    "        if epoch == 0:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        elif epoch == 35:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        # training\n",
    "        model.train() # set the model to training mode\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(inputs) \n",
    "    \n",
    "            batch_loss = criterion(outputs, labels)\n",
    "            _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "            (batch_loss + cal_regularization(model, weight_decay_l1, weight_decay_l2)).backward() \n",
    "\n",
    "            optimizer.step() \n",
    "    \n",
    "            train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "            train_loss += batch_loss.item()\n",
    "    \n",
    "        # validation\n",
    "        if len(valid_dataset) > 0:\n",
    "            model.eval() # set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(valid_dataloader):\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    batch_loss = criterion(outputs, labels) \n",
    "                    _, val_pred = torch.max(outputs, 1) \n",
    "                \n",
    "                    val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "                    val_loss += batch_loss.item()\n",
    "    \n",
    "                print (\"[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}\".format(\n",
    "                    epoch + 1, num_epoch, train_acc / len(train_dataset), train_loss / len(train_dataloader), val_acc / len(valid_dataset), val_loss / len(valid_dataloader)\n",
    "                ))\n",
    "    \n",
    "                # if the model improves, save a checkpoint at this epoch\n",
    "                if val_acc > best_acc:\n",
    "                    best_acc = val_acc\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    print (\"saving model with acc {:.3f}\".format(best_acc / len(valid_dataset)))\n",
    "        else:\n",
    "            print(\"[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}\".format(\n",
    "                epoch + 1, num_epoch, train_acc / len(train_dataset), train_loss / len(train_dataloaders)\n",
    "            ))\n",
    "\n",
    "    # if not validating, save the last epoch\n",
    "    if len(valid_dataset) == 0:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(\"saving model at last epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# get device \n",
    "device = get_device()\n",
    "print(f\"DEVICE: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset = TIMITDataset(valid_x, valid_y)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "del valid_x, valid_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "\n",
      "   class   count\n",
      "       6    7644\n",
      "      10    3637\n",
      "      11    3006\n",
      "      15    3096\n",
      "      20    4866\n",
      "      21    5650\n",
      "      22    5665\n",
      "      23    3603\n",
      "      24    7764\n",
      "      25    5084\n",
      "      26    5473\n",
      "      27    6380\n",
      "      28    3128\n",
      "      32    2773\n",
      "      34    5926\n",
      "      37    1556\n",
      "\n",
      " (813210, 429) (813210,)\n"
     ]
    }
   ],
   "source": [
    "print (\"Sample data:\")\n",
    "print (\"\\n   class   count\")\n",
    "for i in range(len(train_class)):\n",
    "    if (train_class[i].shape[0] < number):\n",
    "        print (\"{:8d}\".format(i), end='')\n",
    "        print (\"{:8d}\".format(number - train_class[i].shape[0]))\n",
    "\n",
    "        id = np.random.choice(train_class[i], size=number-train_class[i].shape[0])\n",
    "        train_x = np.vstack((train_x, train_x[id]))\n",
    "        label = np.empty((id.shape[0]), dtype=int)\n",
    "        train_y = np.append(train_y, label)\n",
    "        train_y[-id.shape[0]:] = int(i)\n",
    "\n",
    "print (\"\\n\", train_x.shape, train_y.shape)\n",
    "train_dataset = TIMITDataset(train_x, train_y)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/200] Train Acc: 0.315937 Loss: 2.556726 | Val Acc: 0.498260 loss: 1.699729\n",
      "saving model with acc 0.498\n",
      "[002/200] Train Acc: 0.462841 Loss: 1.865155 | Val Acc: 0.582996 loss: 1.412998\n",
      "saving model with acc 0.583\n",
      "[003/200] Train Acc: 0.525238 Loss: 1.631198 | Val Acc: 0.624327 loss: 1.261590\n",
      "saving model with acc 0.624\n",
      "[004/200] Train Acc: 0.562797 Loss: 1.492708 | Val Acc: 0.650279 loss: 1.162712\n",
      "saving model with acc 0.650\n",
      "[005/200] Train Acc: 0.589197 Loss: 1.397958 | Val Acc: 0.668806 loss: 1.096518\n",
      "saving model with acc 0.669\n",
      "[006/200] Train Acc: 0.607929 Loss: 1.330902 | Val Acc: 0.680541 loss: 1.051722\n",
      "saving model with acc 0.681\n",
      "[007/200] Train Acc: 0.622641 Loss: 1.276725 | Val Acc: 0.691159 loss: 1.009632\n",
      "saving model with acc 0.691\n",
      "[008/200] Train Acc: 0.634000 Loss: 1.235490 | Val Acc: 0.699241 loss: 0.981304\n",
      "saving model with acc 0.699\n",
      "[009/200] Train Acc: 0.644177 Loss: 1.199961 | Val Acc: 0.705653 loss: 0.959631\n",
      "saving model with acc 0.706\n",
      "[010/200] Train Acc: 0.651805 Loss: 1.172231 | Val Acc: 0.710927 loss: 0.936403\n",
      "saving model with acc 0.711\n",
      "[011/200] Train Acc: 0.658527 Loss: 1.146148 | Val Acc: 0.715513 loss: 0.920489\n",
      "saving model with acc 0.716\n",
      "[012/200] Train Acc: 0.664592 Loss: 1.125393 | Val Acc: 0.721129 loss: 0.901104\n",
      "saving model with acc 0.721\n",
      "[013/200] Train Acc: 0.670436 Loss: 1.105733 | Val Acc: 0.723356 loss: 0.889244\n",
      "saving model with acc 0.723\n",
      "[014/200] Train Acc: 0.674941 Loss: 1.087367 | Val Acc: 0.726966 loss: 0.879068\n",
      "saving model with acc 0.727\n",
      "[015/200] Train Acc: 0.679896 Loss: 1.070568 | Val Acc: 0.728923 loss: 0.867834\n",
      "saving model with acc 0.729\n",
      "[016/200] Train Acc: 0.683982 Loss: 1.055769 | Val Acc: 0.735904 loss: 0.849311\n",
      "saving model with acc 0.736\n",
      "[017/200] Train Acc: 0.688709 Loss: 1.040532 | Val Acc: 0.736864 loss: 0.840470\n",
      "saving model with acc 0.737\n",
      "[018/200] Train Acc: 0.692038 Loss: 1.025578 | Val Acc: 0.739373 loss: 0.831677\n",
      "saving model with acc 0.739\n",
      "[019/200] Train Acc: 0.696411 Loss: 1.011579 | Val Acc: 0.743146 loss: 0.820938\n",
      "saving model with acc 0.743\n",
      "[020/200] Train Acc: 0.698871 Loss: 1.001250 | Val Acc: 0.746165 loss: 0.812151\n",
      "saving model with acc 0.746\n",
      "[021/200] Train Acc: 0.703112 Loss: 0.987397 | Val Acc: 0.748339 loss: 0.802448\n",
      "saving model with acc 0.748\n",
      "[022/200] Train Acc: 0.706049 Loss: 0.977649 | Val Acc: 0.751282 loss: 0.790532\n",
      "saving model with acc 0.751\n",
      "[023/200] Train Acc: 0.708973 Loss: 0.967472 | Val Acc: 0.752642 loss: 0.784943\n",
      "saving model with acc 0.753\n",
      "[024/200] Train Acc: 0.712201 Loss: 0.954922 | Val Acc: 0.755022 loss: 0.778344\n",
      "saving model with acc 0.755\n",
      "[025/200] Train Acc: 0.714359 Loss: 0.947960 | Val Acc: 0.757326 loss: 0.769809\n",
      "saving model with acc 0.757\n",
      "[026/200] Train Acc: 0.716044 Loss: 0.941646 | Val Acc: 0.760545 loss: 0.759756\n",
      "saving model with acc 0.761\n",
      "[027/200] Train Acc: 0.718998 Loss: 0.932522 | Val Acc: 0.760746 loss: 0.759350\n",
      "saving model with acc 0.761\n",
      "[028/200] Train Acc: 0.720241 Loss: 0.926474 | Val Acc: 0.762063 loss: 0.752653\n",
      "saving model with acc 0.762\n",
      "[029/200] Train Acc: 0.722265 Loss: 0.918225 | Val Acc: 0.764551 loss: 0.745608\n",
      "saving model with acc 0.765\n",
      "[030/200] Train Acc: 0.724365 Loss: 0.911502 | Val Acc: 0.768042 loss: 0.737108\n",
      "saving model with acc 0.768\n",
      "[031/200] Train Acc: 0.725362 Loss: 0.905632 | Val Acc: 0.767933 loss: 0.733226\n",
      "[032/200] Train Acc: 0.727278 Loss: 0.899759 | Val Acc: 0.770795 loss: 0.725977\n",
      "saving model with acc 0.771\n",
      "[033/200] Train Acc: 0.728094 Loss: 0.895636 | Val Acc: 0.769993 loss: 0.725738\n",
      "[034/200] Train Acc: 0.729797 Loss: 0.890853 | Val Acc: 0.771012 loss: 0.720212\n",
      "saving model with acc 0.771\n",
      "[035/200] Train Acc: 0.731513 Loss: 0.886216 | Val Acc: 0.772714 loss: 0.716678\n",
      "saving model with acc 0.773\n",
      "[036/200] Train Acc: 0.740901 Loss: 0.854868 | Val Acc: 0.776969 loss: 0.702889\n",
      "saving model with acc 0.777\n",
      "[037/200] Train Acc: 0.743462 Loss: 0.844083 | Val Acc: 0.778709 loss: 0.697560\n",
      "saving model with acc 0.779\n",
      "[038/200] Train Acc: 0.746731 Loss: 0.835102 | Val Acc: 0.779695 loss: 0.694299\n",
      "saving model with acc 0.780\n",
      "[039/200] Train Acc: 0.748261 Loss: 0.830446 | Val Acc: 0.780644 loss: 0.691116\n",
      "saving model with acc 0.781\n",
      "[040/200] Train Acc: 0.749788 Loss: 0.826551 | Val Acc: 0.781647 loss: 0.688861\n",
      "saving model with acc 0.782\n",
      "[041/200] Train Acc: 0.750840 Loss: 0.821030 | Val Acc: 0.781977 loss: 0.686901\n",
      "saving model with acc 0.782\n",
      "[042/200] Train Acc: 0.751791 Loss: 0.818170 | Val Acc: 0.782742 loss: 0.684941\n",
      "saving model with acc 0.783\n",
      "[043/200] Train Acc: 0.753252 Loss: 0.813473 | Val Acc: 0.783284 loss: 0.683440\n",
      "saving model with acc 0.783\n",
      "[044/200] Train Acc: 0.753824 Loss: 0.811156 | Val Acc: 0.783734 loss: 0.682393\n",
      "saving model with acc 0.784\n",
      "[045/200] Train Acc: 0.754980 Loss: 0.808344 | Val Acc: 0.784156 loss: 0.680138\n",
      "saving model with acc 0.784\n",
      "[046/200] Train Acc: 0.756204 Loss: 0.804280 | Val Acc: 0.784108 loss: 0.679391\n",
      "[047/200] Train Acc: 0.756128 Loss: 0.803270 | Val Acc: 0.784823 loss: 0.678009\n",
      "saving model with acc 0.785\n",
      "[048/200] Train Acc: 0.757106 Loss: 0.800396 | Val Acc: 0.785007 loss: 0.677193\n",
      "saving model with acc 0.785\n",
      "[049/200] Train Acc: 0.758184 Loss: 0.797705 | Val Acc: 0.786167 loss: 0.674411\n",
      "saving model with acc 0.786\n",
      "[050/200] Train Acc: 0.758045 Loss: 0.796467 | Val Acc: 0.786194 loss: 0.673928\n",
      "saving model with acc 0.786\n",
      "[051/200] Train Acc: 0.759100 Loss: 0.793455 | Val Acc: 0.786688 loss: 0.672767\n",
      "saving model with acc 0.787\n",
      "[052/200] Train Acc: 0.759911 Loss: 0.790925 | Val Acc: 0.786774 loss: 0.672380\n",
      "saving model with acc 0.787\n",
      "[053/200] Train Acc: 0.760083 Loss: 0.790622 | Val Acc: 0.787241 loss: 0.671186\n",
      "saving model with acc 0.787\n",
      "[054/200] Train Acc: 0.760870 Loss: 0.788342 | Val Acc: 0.787501 loss: 0.670095\n",
      "saving model with acc 0.788\n",
      "[055/200] Train Acc: 0.761871 Loss: 0.786013 | Val Acc: 0.787745 loss: 0.669341\n",
      "saving model with acc 0.788\n",
      "[056/200] Train Acc: 0.762125 Loss: 0.784046 | Val Acc: 0.788108 loss: 0.668477\n",
      "saving model with acc 0.788\n",
      "[057/200] Train Acc: 0.762785 Loss: 0.781605 | Val Acc: 0.788802 loss: 0.666829\n",
      "saving model with acc 0.789\n",
      "[058/200] Train Acc: 0.763505 Loss: 0.779469 | Val Acc: 0.788845 loss: 0.666366\n",
      "saving model with acc 0.789\n",
      "[059/200] Train Acc: 0.763781 Loss: 0.779973 | Val Acc: 0.789490 loss: 0.665237\n",
      "saving model with acc 0.789\n",
      "[060/200] Train Acc: 0.764159 Loss: 0.777101 | Val Acc: 0.789512 loss: 0.664268\n",
      "saving model with acc 0.790\n",
      "[061/200] Train Acc: 0.764561 Loss: 0.775301 | Val Acc: 0.789853 loss: 0.663982\n",
      "saving model with acc 0.790\n",
      "[062/200] Train Acc: 0.764896 Loss: 0.774993 | Val Acc: 0.789864 loss: 0.663087\n",
      "saving model with acc 0.790\n",
      "[063/200] Train Acc: 0.765134 Loss: 0.773551 | Val Acc: 0.790298 loss: 0.662235\n",
      "saving model with acc 0.790\n",
      "[064/200] Train Acc: 0.765942 Loss: 0.771934 | Val Acc: 0.790715 loss: 0.660948\n",
      "saving model with acc 0.791\n",
      "[065/200] Train Acc: 0.765785 Loss: 0.770686 | Val Acc: 0.790813 loss: 0.660558\n",
      "saving model with acc 0.791\n",
      "[066/200] Train Acc: 0.766566 Loss: 0.768488 | Val Acc: 0.790991 loss: 0.660115\n",
      "saving model with acc 0.791\n",
      "[067/200] Train Acc: 0.766989 Loss: 0.767333 | Val Acc: 0.790780 loss: 0.659868\n",
      "[068/200] Train Acc: 0.767434 Loss: 0.765765 | Val Acc: 0.791132 loss: 0.658918\n",
      "saving model with acc 0.791\n",
      "[069/200] Train Acc: 0.767438 Loss: 0.765232 | Val Acc: 0.791170 loss: 0.658304\n",
      "saving model with acc 0.791\n",
      "[070/200] Train Acc: 0.767930 Loss: 0.764341 | Val Acc: 0.791387 loss: 0.657133\n",
      "saving model with acc 0.791\n",
      "[071/200] Train Acc: 0.768335 Loss: 0.763065 | Val Acc: 0.791842 loss: 0.656312\n",
      "saving model with acc 0.792\n",
      "[072/200] Train Acc: 0.768244 Loss: 0.761954 | Val Acc: 0.792010 loss: 0.655961\n",
      "saving model with acc 0.792\n",
      "[073/200] Train Acc: 0.768634 Loss: 0.762548 | Val Acc: 0.792108 loss: 0.655252\n",
      "saving model with acc 0.792\n",
      "[074/200] Train Acc: 0.769531 Loss: 0.758752 | Val Acc: 0.792509 loss: 0.654919\n",
      "saving model with acc 0.793\n",
      "[075/200] Train Acc: 0.770013 Loss: 0.757275 | Val Acc: 0.792932 loss: 0.653671\n",
      "saving model with acc 0.793\n",
      "[076/200] Train Acc: 0.770154 Loss: 0.756358 | Val Acc: 0.792666 loss: 0.653724\n",
      "[077/200] Train Acc: 0.770496 Loss: 0.755400 | Val Acc: 0.793154 loss: 0.652204\n",
      "saving model with acc 0.793\n",
      "[078/200] Train Acc: 0.770699 Loss: 0.754055 | Val Acc: 0.793235 loss: 0.651793\n",
      "saving model with acc 0.793\n",
      "[079/200] Train Acc: 0.770572 Loss: 0.754251 | Val Acc: 0.793420 loss: 0.651248\n",
      "saving model with acc 0.793\n",
      "[080/200] Train Acc: 0.771412 Loss: 0.752774 | Val Acc: 0.793864 loss: 0.650662\n",
      "saving model with acc 0.794\n",
      "[081/200] Train Acc: 0.772290 Loss: 0.749870 | Val Acc: 0.793745 loss: 0.650013\n",
      "[082/200] Train Acc: 0.771828 Loss: 0.750457 | Val Acc: 0.794189 loss: 0.649545\n",
      "saving model with acc 0.794\n",
      "[083/200] Train Acc: 0.771965 Loss: 0.749833 | Val Acc: 0.794363 loss: 0.649345\n",
      "saving model with acc 0.794\n",
      "[084/200] Train Acc: 0.772583 Loss: 0.748289 | Val Acc: 0.794206 loss: 0.648407\n",
      "[085/200] Train Acc: 0.772752 Loss: 0.748379 | Val Acc: 0.794227 loss: 0.648776\n",
      "[086/200] Train Acc: 0.773927 Loss: 0.744964 | Val Acc: 0.794509 loss: 0.647795\n",
      "saving model with acc 0.795\n",
      "[087/200] Train Acc: 0.773673 Loss: 0.745648 | Val Acc: 0.794352 loss: 0.648515\n",
      "[088/200] Train Acc: 0.774428 Loss: 0.743362 | Val Acc: 0.795436 loss: 0.645880\n",
      "saving model with acc 0.795\n",
      "[089/200] Train Acc: 0.774579 Loss: 0.743242 | Val Acc: 0.795431 loss: 0.645342\n",
      "[090/200] Train Acc: 0.774575 Loss: 0.741973 | Val Acc: 0.795723 loss: 0.645474\n",
      "saving model with acc 0.796\n",
      "[091/200] Train Acc: 0.774141 Loss: 0.741904 | Val Acc: 0.795745 loss: 0.644619\n",
      "saving model with acc 0.796\n",
      "[092/200] Train Acc: 0.774526 Loss: 0.742281 | Val Acc: 0.796103 loss: 0.643657\n",
      "saving model with acc 0.796\n",
      "[093/200] Train Acc: 0.775337 Loss: 0.739205 | Val Acc: 0.795918 loss: 0.643584\n",
      "[094/200] Train Acc: 0.775447 Loss: 0.738787 | Val Acc: 0.796184 loss: 0.643454\n",
      "saving model with acc 0.796\n",
      "[095/200] Train Acc: 0.775950 Loss: 0.738037 | Val Acc: 0.795940 loss: 0.642684\n",
      "[096/200] Train Acc: 0.775883 Loss: 0.736328 | Val Acc: 0.796471 loss: 0.642045\n",
      "saving model with acc 0.796\n",
      "[097/200] Train Acc: 0.776412 Loss: 0.736421 | Val Acc: 0.796423 loss: 0.641486\n",
      "[098/200] Train Acc: 0.776562 Loss: 0.735330 | Val Acc: 0.797003 loss: 0.641123\n",
      "saving model with acc 0.797\n",
      "[099/200] Train Acc: 0.776485 Loss: 0.735041 | Val Acc: 0.797165 loss: 0.640581\n",
      "saving model with acc 0.797\n",
      "[100/200] Train Acc: 0.776303 Loss: 0.734754 | Val Acc: 0.797366 loss: 0.639812\n",
      "saving model with acc 0.797\n",
      "[101/200] Train Acc: 0.777502 Loss: 0.732586 | Val Acc: 0.797550 loss: 0.639072\n",
      "saving model with acc 0.798\n",
      "[102/200] Train Acc: 0.777938 Loss: 0.730750 | Val Acc: 0.797339 loss: 0.639368\n",
      "[103/200] Train Acc: 0.777770 Loss: 0.730436 | Val Acc: 0.797572 loss: 0.638862\n",
      "saving model with acc 0.798\n",
      "[104/200] Train Acc: 0.778161 Loss: 0.729663 | Val Acc: 0.797615 loss: 0.638329\n",
      "saving model with acc 0.798\n",
      "[105/200] Train Acc: 0.778518 Loss: 0.729007 | Val Acc: 0.798141 loss: 0.636767\n",
      "saving model with acc 0.798\n",
      "[106/200] Train Acc: 0.778855 Loss: 0.729210 | Val Acc: 0.797902 loss: 0.637620\n",
      "[107/200] Train Acc: 0.778887 Loss: 0.727718 | Val Acc: 0.797778 loss: 0.637993\n",
      "[108/200] Train Acc: 0.779374 Loss: 0.727551 | Val Acc: 0.798222 loss: 0.636490\n",
      "saving model with acc 0.798\n",
      "[109/200] Train Acc: 0.778795 Loss: 0.727199 | Val Acc: 0.798531 loss: 0.636347\n",
      "saving model with acc 0.799\n",
      "[110/200] Train Acc: 0.779431 Loss: 0.725674 | Val Acc: 0.798661 loss: 0.635145\n",
      "saving model with acc 0.799\n",
      "[111/200] Train Acc: 0.779223 Loss: 0.725539 | Val Acc: 0.799214 loss: 0.634860\n",
      "saving model with acc 0.799\n",
      "[112/200] Train Acc: 0.779389 Loss: 0.725348 | Val Acc: 0.799333 loss: 0.634314\n",
      "saving model with acc 0.799\n",
      "[113/200] Train Acc: 0.780074 Loss: 0.722278 | Val Acc: 0.799317 loss: 0.634189\n",
      "[114/200] Train Acc: 0.780171 Loss: 0.722755 | Val Acc: 0.799274 loss: 0.633997\n",
      "[115/200] Train Acc: 0.780374 Loss: 0.722564 | Val Acc: 0.799718 loss: 0.633388\n",
      "saving model with acc 0.800\n",
      "[116/200] Train Acc: 0.780672 Loss: 0.719823 | Val Acc: 0.799664 loss: 0.632712\n",
      "[117/200] Train Acc: 0.781221 Loss: 0.720248 | Val Acc: 0.799919 loss: 0.632233\n",
      "saving model with acc 0.800\n",
      "[118/200] Train Acc: 0.781028 Loss: 0.719221 | Val Acc: 0.800043 loss: 0.631730\n",
      "saving model with acc 0.800\n",
      "[119/200] Train Acc: 0.781122 Loss: 0.718499 | Val Acc: 0.800271 loss: 0.631532\n",
      "saving model with acc 0.800\n",
      "[120/200] Train Acc: 0.781475 Loss: 0.717528 | Val Acc: 0.800255 loss: 0.631701\n",
      "[121/200] Train Acc: 0.781557 Loss: 0.717398 | Val Acc: 0.800531 loss: 0.630811\n",
      "saving model with acc 0.801\n",
      "[122/200] Train Acc: 0.781973 Loss: 0.716671 | Val Acc: 0.800721 loss: 0.629979\n",
      "saving model with acc 0.801\n",
      "[123/200] Train Acc: 0.782237 Loss: 0.715467 | Val Acc: 0.800894 loss: 0.629747\n",
      "saving model with acc 0.801\n",
      "[124/200] Train Acc: 0.782240 Loss: 0.715168 | Val Acc: 0.801014 loss: 0.629112\n",
      "saving model with acc 0.801\n",
      "[125/200] Train Acc: 0.782365 Loss: 0.715244 | Val Acc: 0.801046 loss: 0.629126\n",
      "saving model with acc 0.801\n",
      "[126/200] Train Acc: 0.782793 Loss: 0.714306 | Val Acc: 0.801035 loss: 0.628781\n",
      "[127/200] Train Acc: 0.782883 Loss: 0.713238 | Val Acc: 0.801057 loss: 0.628392\n",
      "saving model with acc 0.801\n",
      "[128/200] Train Acc: 0.783395 Loss: 0.711691 | Val Acc: 0.801084 loss: 0.627825\n",
      "saving model with acc 0.801\n",
      "[129/200] Train Acc: 0.783125 Loss: 0.712363 | Val Acc: 0.801566 loss: 0.627384\n",
      "saving model with acc 0.802\n",
      "[130/200] Train Acc: 0.783068 Loss: 0.711164 | Val Acc: 0.801566 loss: 0.626286\n",
      "[131/200] Train Acc: 0.783656 Loss: 0.710193 | Val Acc: 0.802011 loss: 0.625906\n",
      "saving model with acc 0.802\n",
      "[132/200] Train Acc: 0.783705 Loss: 0.710045 | Val Acc: 0.801778 loss: 0.626046\n",
      "[133/200] Train Acc: 0.783962 Loss: 0.708759 | Val Acc: 0.801913 loss: 0.626048\n",
      "[134/200] Train Acc: 0.784474 Loss: 0.707574 | Val Acc: 0.802119 loss: 0.624882\n",
      "saving model with acc 0.802\n",
      "[135/200] Train Acc: 0.784720 Loss: 0.707259 | Val Acc: 0.802271 loss: 0.625206\n",
      "saving model with acc 0.802\n",
      "[136/200] Train Acc: 0.784462 Loss: 0.706804 | Val Acc: 0.802190 loss: 0.624766\n",
      "[137/200] Train Acc: 0.783983 Loss: 0.708180 | Val Acc: 0.802537 loss: 0.623976\n",
      "saving model with acc 0.803\n",
      "[138/200] Train Acc: 0.784923 Loss: 0.705938 | Val Acc: 0.802862 loss: 0.623365\n",
      "saving model with acc 0.803\n",
      "[139/200] Train Acc: 0.785589 Loss: 0.704916 | Val Acc: 0.802802 loss: 0.623214\n",
      "[140/200] Train Acc: 0.785010 Loss: 0.704151 | Val Acc: 0.802894 loss: 0.622842\n",
      "saving model with acc 0.803\n",
      "[141/200] Train Acc: 0.785246 Loss: 0.705129 | Val Acc: 0.803144 loss: 0.622876\n",
      "saving model with acc 0.803\n",
      "[142/200] Train Acc: 0.786139 Loss: 0.702838 | Val Acc: 0.803545 loss: 0.622040\n",
      "saving model with acc 0.804\n",
      "[143/200] Train Acc: 0.786368 Loss: 0.702417 | Val Acc: 0.803534 loss: 0.620860\n",
      "[144/200] Train Acc: 0.785407 Loss: 0.704268 | Val Acc: 0.803621 loss: 0.621212\n",
      "saving model with acc 0.804\n",
      "[145/200] Train Acc: 0.786328 Loss: 0.701677 | Val Acc: 0.803437 loss: 0.621792\n",
      "[146/200] Train Acc: 0.786636 Loss: 0.700913 | Val Acc: 0.804277 loss: 0.619945\n",
      "saving model with acc 0.804\n",
      "[147/200] Train Acc: 0.786489 Loss: 0.700566 | Val Acc: 0.804098 loss: 0.620035\n",
      "[148/200] Train Acc: 0.787048 Loss: 0.699437 | Val Acc: 0.804412 loss: 0.619040\n",
      "saving model with acc 0.804\n",
      "[149/200] Train Acc: 0.787381 Loss: 0.698243 | Val Acc: 0.804092 loss: 0.619298\n",
      "[150/200] Train Acc: 0.787231 Loss: 0.698609 | Val Acc: 0.804385 loss: 0.618589\n",
      "[151/200] Train Acc: 0.786894 Loss: 0.699610 | Val Acc: 0.804434 loss: 0.618638\n",
      "saving model with acc 0.804\n",
      "[152/200] Train Acc: 0.787565 Loss: 0.697582 | Val Acc: 0.804353 loss: 0.618651\n",
      "[153/200] Train Acc: 0.787725 Loss: 0.696759 | Val Acc: 0.804710 loss: 0.617168\n",
      "saving model with acc 0.805\n",
      "[154/200] Train Acc: 0.788248 Loss: 0.695936 | Val Acc: 0.805041 loss: 0.617146\n",
      "saving model with acc 0.805\n",
      "[155/200] Train Acc: 0.787761 Loss: 0.696568 | Val Acc: 0.805214 loss: 0.616499\n",
      "saving model with acc 0.805\n",
      "[156/200] Train Acc: 0.788414 Loss: 0.694751 | Val Acc: 0.804895 loss: 0.616659\n",
      "[157/200] Train Acc: 0.788384 Loss: 0.694366 | Val Acc: 0.804857 loss: 0.616247\n",
      "[158/200] Train Acc: 0.788619 Loss: 0.692368 | Val Acc: 0.804960 loss: 0.615651\n",
      "[159/200] Train Acc: 0.789181 Loss: 0.693581 | Val Acc: 0.805437 loss: 0.616146\n",
      "saving model with acc 0.805\n",
      "[160/200] Train Acc: 0.788764 Loss: 0.693841 | Val Acc: 0.805366 loss: 0.615802\n",
      "[161/200] Train Acc: 0.788647 Loss: 0.691803 | Val Acc: 0.805572 loss: 0.614385\n",
      "saving model with acc 0.806\n",
      "[162/200] Train Acc: 0.789223 Loss: 0.690326 | Val Acc: 0.805762 loss: 0.614216\n",
      "saving model with acc 0.806\n",
      "[163/200] Train Acc: 0.789041 Loss: 0.690944 | Val Acc: 0.805691 loss: 0.614438\n",
      "[164/200] Train Acc: 0.789539 Loss: 0.689608 | Val Acc: 0.805968 loss: 0.613167\n",
      "saving model with acc 0.806\n",
      "[165/200] Train Acc: 0.789754 Loss: 0.690271 | Val Acc: 0.806087 loss: 0.613613\n",
      "saving model with acc 0.806\n",
      "[166/200] Train Acc: 0.789976 Loss: 0.689633 | Val Acc: 0.805968 loss: 0.613626\n",
      "[167/200] Train Acc: 0.790444 Loss: 0.688950 | Val Acc: 0.806206 loss: 0.612565\n",
      "saving model with acc 0.806\n",
      "[168/200] Train Acc: 0.790261 Loss: 0.687351 | Val Acc: 0.806499 loss: 0.612179\n",
      "saving model with acc 0.806\n",
      "[169/200] Train Acc: 0.790809 Loss: 0.687019 | Val Acc: 0.806645 loss: 0.611432\n",
      "saving model with acc 0.807\n",
      "[170/200] Train Acc: 0.791098 Loss: 0.686687 | Val Acc: 0.806564 loss: 0.610695\n",
      "[171/200] Train Acc: 0.791380 Loss: 0.685686 | Val Acc: 0.806532 loss: 0.611441\n",
      "[172/200] Train Acc: 0.791428 Loss: 0.684451 | Val Acc: 0.806548 loss: 0.611819\n",
      "[173/200] Train Acc: 0.791076 Loss: 0.685164 | Val Acc: 0.807312 loss: 0.609749\n",
      "saving model with acc 0.807\n",
      "[174/200] Train Acc: 0.791085 Loss: 0.685060 | Val Acc: 0.807090 loss: 0.610348\n",
      "[175/200] Train Acc: 0.791200 Loss: 0.684534 | Val Acc: 0.807328 loss: 0.609697\n",
      "saving model with acc 0.807\n",
      "[176/200] Train Acc: 0.791638 Loss: 0.683180 | Val Acc: 0.807231 loss: 0.609751\n",
      "[177/200] Train Acc: 0.792045 Loss: 0.682616 | Val Acc: 0.807334 loss: 0.609658\n",
      "saving model with acc 0.807\n",
      "[178/200] Train Acc: 0.792036 Loss: 0.681778 | Val Acc: 0.807464 loss: 0.608749\n",
      "saving model with acc 0.807\n",
      "[179/200] Train Acc: 0.792632 Loss: 0.681316 | Val Acc: 0.807914 loss: 0.608291\n",
      "saving model with acc 0.808\n",
      "[180/200] Train Acc: 0.791574 Loss: 0.682456 | Val Acc: 0.808190 loss: 0.607413\n",
      "saving model with acc 0.808\n",
      "[181/200] Train Acc: 0.792831 Loss: 0.679784 | Val Acc: 0.807822 loss: 0.607737\n",
      "[182/200] Train Acc: 0.792757 Loss: 0.680741 | Val Acc: 0.808017 loss: 0.606602\n",
      "[183/200] Train Acc: 0.793104 Loss: 0.678609 | Val Acc: 0.808288 loss: 0.606428\n",
      "saving model with acc 0.808\n",
      "[184/200] Train Acc: 0.792736 Loss: 0.679540 | Val Acc: 0.808342 loss: 0.605631\n",
      "saving model with acc 0.808\n",
      "[185/200] Train Acc: 0.793026 Loss: 0.678360 | Val Acc: 0.808082 loss: 0.606055\n",
      "[186/200] Train Acc: 0.793227 Loss: 0.678662 | Val Acc: 0.808456 loss: 0.605608\n",
      "saving model with acc 0.808\n",
      "[187/200] Train Acc: 0.793480 Loss: 0.677066 | Val Acc: 0.808407 loss: 0.605469\n",
      "[188/200] Train Acc: 0.793480 Loss: 0.677476 | Val Acc: 0.808635 loss: 0.605322\n",
      "saving model with acc 0.809\n",
      "[189/200] Train Acc: 0.793812 Loss: 0.676228 | Val Acc: 0.808689 loss: 0.605005\n",
      "saving model with acc 0.809\n",
      "[190/200] Train Acc: 0.793478 Loss: 0.676744 | Val Acc: 0.808374 loss: 0.605377\n",
      "[191/200] Train Acc: 0.794134 Loss: 0.675384 | Val Acc: 0.808710 loss: 0.604401\n",
      "saving model with acc 0.809\n",
      "[192/200] Train Acc: 0.794267 Loss: 0.674539 | Val Acc: 0.809085 loss: 0.603653\n",
      "saving model with acc 0.809\n",
      "[193/200] Train Acc: 0.794410 Loss: 0.673916 | Val Acc: 0.809518 loss: 0.602919\n",
      "saving model with acc 0.810\n",
      "[194/200] Train Acc: 0.794063 Loss: 0.674335 | Val Acc: 0.809361 loss: 0.602534\n",
      "[195/200] Train Acc: 0.793890 Loss: 0.673613 | Val Acc: 0.809334 loss: 0.603100\n",
      "[196/200] Train Acc: 0.794993 Loss: 0.673921 | Val Acc: 0.809702 loss: 0.602483\n",
      "saving model with acc 0.810\n",
      "[197/200] Train Acc: 0.794749 Loss: 0.673021 | Val Acc: 0.809757 loss: 0.602150\n",
      "saving model with acc 0.810\n",
      "[198/200] Train Acc: 0.795315 Loss: 0.670811 | Val Acc: 0.810234 loss: 0.600546\n",
      "saving model with acc 0.810\n",
      "[199/200] Train Acc: 0.795274 Loss: 0.670086 | Val Acc: 0.810028 loss: 0.601340\n",
      "[200/200] Train Acc: 0.795855 Loss: 0.668577 | Val Acc: 0.810082 loss: 0.601034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(num_epoch, learning_rate, weight_decay_l1, weight_decay_l2, train_dataset, train_dataloader, valid_dataset, valid_dataloader)\n",
    "\n",
    "del train_x, train_y, train_dataset, train_dataloader, valid_dataset, valid_dataloader\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create testing dataset\n",
    "test_dataset = TIMITDataset(test_data, None)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# create model and load weights from checkpoint\n",
    "model = Classifier().to(device)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs = data\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, test_pred = torch.max(outputs, 1) \n",
    "\n",
    "        for y in test_pred.cpu().numpy():\n",
    "            predict.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prediction.csv\", 'w') as f:\n",
    "    f.write(\"Id,Class\\n\")\n",
    "    for i, y in enumerate(predict):\n",
    "        f.write(\"{},{}\\n\".format(i, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56215155276dfdf8111aee81c8ed51390ea50134a2a70f555fa6102f0dd24350"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
