{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2949c89c",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002a1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "from os.path import join\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba92278",
   "metadata": {},
   "source": [
    "# Read file and Img Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d069fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 96, 96, 3)\n",
      "(1,)\n",
      "(1, 96, 96, 3)\n",
      "(1, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ESALAB\\Desktop\\CIFAR100_testImage\\410785018_Cifar100_CNN.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ESALAB/Desktop/CIFAR100_testImage/410785018_Cifar100_CNN.ipynb#W3sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m########################################################################################\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ESALAB/Desktop/CIFAR100_testImage/410785018_Cifar100_CNN.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ESALAB/Desktop/CIFAR100_testImage/410785018_Cifar100_CNN.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# normalization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ESALAB/Desktop/CIFAR100_testImage/410785018_Cifar100_CNN.ipynb#W3sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m train_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(train_data)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ESALAB/Desktop/CIFAR100_testImage/410785018_Cifar100_CNN.ipynb#W3sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m train_data,val_data,train_label,val_label \u001b[39m=\u001b[39m train_test_split(train_data, train_label, test_size \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m, random_state \u001b[39m=\u001b[39;49m \u001b[39m42\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ESALAB/Desktop/CIFAR100_testImage/410785018_Cifar100_CNN.ipynb#W3sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m train_datagen\u001b[39m=\u001b[39mImageDataGenerator(rescale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ESALAB/Desktop/CIFAR100_testImage/410785018_Cifar100_CNN.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m                                  \u001b[39m#rotation_range = 15, \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ESALAB/Desktop/CIFAR100_testImage/410785018_Cifar100_CNN.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m                                  width_shift_range \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ESALAB/Desktop/CIFAR100_testImage/410785018_Cifar100_CNN.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m                                  zoom_range \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ESALAB/Desktop/CIFAR100_testImage/410785018_Cifar100_CNN.ipynb#W3sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m                                  )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ESALAB/Desktop/CIFAR100_testImage/410785018_Cifar100_CNN.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m val_datagen \u001b[39m=\u001b[39m ImageDataGenerator(rescale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2420\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2417\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2419\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 2420\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2421\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[0;32m   2422\u001b[0m )\n\u001b[0;32m   2424\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m   2425\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2098\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2095\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[0;32m   2097\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2098\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2099\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2100\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2101\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2102\u001b[0m     )\n\u001b[0;32m   2104\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=1, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "#######################################################################################\n",
    "traindata_path = r\"C:\\Users\\ESALAB\\Desktop\\CIFAR100_testImage\\Training_data\"\n",
    "numbers_of_traindata = 0\n",
    "numbers_of_testdata = 0\n",
    "\n",
    "train_data = []\n",
    "train_label = []\n",
    "\n",
    "label_folder = []\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "for root, dirts, files in os.walk(traindata_path):\n",
    "    for dirt in dirts:\n",
    "        label_folder.append(dirt)\n",
    "    numbers_of_traindata += len(files)\n",
    "    \n",
    "for i in range(len(label_folder)):\n",
    "    label_path = traindata_path + r\"\\\\\" + label_folder[i]\n",
    "    trainfileName = [f for f in listdir(label_path) if isfile(join(label_path, f))]\n",
    "    \n",
    "    for j in range(len(trainfileName)):\n",
    "        path2 = label_path + r\"\\\\\" + trainfileName[j]\n",
    "        img = cv2.imread(path2, cv2.IMREAD_COLOR)\n",
    "        # cv2.imshow('My Image', img)\n",
    "        # cv2.waitKey(0)\n",
    "        # original\n",
    "        train_data.append(img)\n",
    "        train_label.append(label_folder[i])\n",
    "        \n",
    "\n",
    "print(np.array(train_data).shape)\n",
    "print(np.array(train_label).shape)\n",
    "\n",
    "########################################################################################\n",
    "#Convert a category vector to a binary (0 or 1) matrix-type representation\n",
    "\n",
    "train_label = to_categorical(train_label)\n",
    "\n",
    "print(np.array(train_data).shape)\n",
    "print(np.array(train_label).shape)\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "# normalization\n",
    "train_data = np.array(train_data).astype('float32')\n",
    "\n",
    "train_data,val_data,train_label,val_label = train_test_split(train_data, train_label, test_size = 0.1, random_state = 42)\n",
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1./255, \n",
    "                                 #rotation_range = 15, \n",
    "                                 width_shift_range = 0.2, \n",
    "                                 height_shift_range = 0.2,\n",
    "                                 shear_range = 0.2,\n",
    "                                 zoom_range = 0.2\n",
    "                                 )\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_datagen.fit(train_data)\n",
    "val_datagen.fit(val_data)\n",
    "\n",
    "\n",
    "LR_function = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                                patience = 2,\n",
    "                                verbose = 1,\n",
    "                                factor = 0.5,\n",
    "                                min_lr = 0.00001)\n",
    "\n",
    "EarlyStop = EarlyStopping(monitor='val_loss',\n",
    "                          patience = 6,\n",
    "                          verbose = 1, \n",
    "                          mode = 'min')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89e25e",
   "metadata": {},
   "source": [
    "# Load Keras Data and Img Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8485d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_label), (test_data, test_label) = keras.datasets.cifar100.load_data()\n",
    "train_data.reshape(50000, 32, 32, 3)\n",
    "test_data.reshape(10000, 32, 32, 3)\n",
    "\n",
    "train_label = to_categorical(train_label, 100)\n",
    "test_label = to_categorical(test_label, 100)\n",
    "\n",
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1./255, \n",
    "                                 #rotation_range = 15, \n",
    "                                 width_shift_range = 0.2, \n",
    "                                 height_shift_range = 0.2,\n",
    "                                 shear_range = 0.2,\n",
    "                                 zoom_range = 0.2\n",
    "                                 )\n",
    "\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_datagen.fit(train_data)\n",
    "test_datagen.fit(test_data)\n",
    "\n",
    "LR_function = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                                patience = 2,\n",
    "                                verbose = 1,\n",
    "                                factor = 0.5,\n",
    "                                min_lr = 0.00001)\n",
    "\n",
    "EarlyStop = EarlyStopping(monitor='val_loss',\n",
    "                          patience = 6,\n",
    "                          verbose = 1, \n",
    "                          mode = 'min')\n",
    "\n",
    "print(np.array(train_data).shape)\n",
    "print(np.array(train_label).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8477587",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# model \n",
    "model = Sequential()\n",
    "\n",
    "#conv1\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv2\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling1\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv3\n",
    "model.add(Conv2D(128,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv4\n",
    "model.add(Conv2D(128,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling2\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv5\n",
    "model.add(Conv2D(256,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv6\n",
    "model.add(Conv2D(256,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv7\n",
    "model.add(Conv2D(256,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling3\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv8\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv9\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling4\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256, kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "# model.add(Dense(64, kernel_initializer=\"he_normal\", activation=\"relu\"))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(30, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3c6ba",
   "metadata": {},
   "source": [
    "# Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988cd094",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# model \n",
    "model = Sequential()\n",
    "\n",
    "#conv1\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv2\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling1\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv3\n",
    "model.add(Conv2D(128,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv4\n",
    "model.add(Conv2D(128,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling2\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv5\n",
    "model.add(Conv2D(256,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv6\n",
    "model.add(Conv2D(256,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv7\n",
    "model.add(Conv2D(256,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling3\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv8\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv9\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling4\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv11\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv12\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv13\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling5\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9b574",
   "metadata": {},
   "source": [
    "# Model-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# model \n",
    "model = Sequential()\n",
    "\n",
    "#conv1\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv2\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling1\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv3\n",
    "model.add(Conv2D(128,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv4\n",
    "model.add(Conv2D(128,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling2\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv5\n",
    "model.add(Conv2D(256,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv6\n",
    "model.add(Conv2D(256,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv7\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling3\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv8\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv9\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling4\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256, kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(30, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d311b2",
   "metadata": {},
   "source": [
    "# Model - 4 (Best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# model \n",
    "model = Sequential()\n",
    "\n",
    "#conv1\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv2\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling1\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv3\n",
    "model.add(Conv2D(256,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv4\n",
    "model.add(Conv2D(256,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling2\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv5\n",
    "model.add(Conv2D(256,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv6\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv7\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling3\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#conv8\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv9\n",
    "model.add(Conv2D(512,(3,3), padding=\"same\", kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#pooling4\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256, kernel_initializer=\"he_normal\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "# model.add(Dense(30, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379db7e",
   "metadata": {},
   "source": [
    "# Evaluation_For_LoadingKerasData(Full Processing)(Batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae278f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 50\n",
    "batch_size = 32\n",
    "\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = \"Adam\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(train_datagen.flow(train_data,train_label, batch_size = batch_size), \n",
    "                              steps_per_epoch = train_data.shape[0] / batch_size , \n",
    "                              validation_data = test_datagen.flow(test_data, test_label, batch_size = batch_size),\n",
    "                              validation_steps = test_data.shape[0] / batch_size,\n",
    "                              epochs = epoch,\n",
    "                              callbacks = [LR_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f7636",
   "metadata": {},
   "source": [
    "# Evaluation_For_LoadingKerasData(Full Processing)(Batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b71c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 50\n",
    "batch_size = 1024\n",
    "\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = \"Adam\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(train_datagen.flow(train_data,train_label, batch_size = batch_size), \n",
    "                              steps_per_epoch = train_data.shape[0] / batch_size , \n",
    "                              validation_data = test_datagen.flow(test_data, test_label, batch_size = batch_size),\n",
    "                              validation_steps = test_data.shape[0] / batch_size,\n",
    "                              epochs = epoch,\n",
    "                              callbacks = [LR_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006fc207",
   "metadata": {},
   "source": [
    "# Evaluation_For_LoadingKerasData(No IMG Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76231241",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 50\n",
    "batch_size = 32\n",
    "\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = \"Adam\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(train_datagen.flow(train_data,train_label, batch_size = batch_size), \n",
    "                              steps_per_epoch = train_data.shape[0] / batch_size , \n",
    "                              validation_data = test_datagen.flow(test_data, test_label, batch_size = batch_size),\n",
    "                              validation_steps = test_data.shape[0] / batch_size,\n",
    "                              epochs = epoch,\n",
    "                              callbacks = [LR_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee1905f",
   "metadata": {},
   "source": [
    "# Evaluation_For_LoadingKerasData(No Reduce Learning Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4290deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 50\n",
    "batch_size = 32\n",
    "\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = \"Adam\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(train_datagen.flow(train_data,train_label, batch_size = batch_size), \n",
    "                    steps_per_epoch = train_data.shape[0] / batch_size , \n",
    "                    validation_data = test_datagen.flow(test_data, test_label, batch_size = batch_size),\n",
    "                    validation_steps = test_data.shape[0] / batch_size,\n",
    "                    epochs = epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ddfd3",
   "metadata": {},
   "source": [
    "# Evaluation_For_Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023ae88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch = 50\n",
    "batch_size = 32\n",
    "\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = \"Adam\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(train_datagen.flow(train_data,train_label, batch_size = batch_size), \n",
    "                    steps_per_epoch = train_data.shape[0] / batch_size , \n",
    "                    validation_data = val_datagen.flow(val_data, val_label),\n",
    "                    epochs = epoch,\n",
    "                    callbacks = [LR_function, EarlyStop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8aa70",
   "metadata": {},
   "source": [
    "# Evaluation_For_Read - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 50\n",
    "batch_size = 32\n",
    "\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = \"Adam\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(train_datagen.flow(train_data,train_label, batch_size = batch_size), \n",
    "                    steps_per_epoch = train_data.shape[0] / batch_size , \n",
    "                    validation_data = val_datagen.flow(val_data, val_label),\n",
    "                    epochs = epoch,\n",
    "                    callbacks = [LR_function, EarlyStop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65e14d",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_path = r\"C:\\Users\\foo\\Desktop\\CNN\\Testing_data-2\"\n",
    "test_data = []\n",
    "\n",
    "for dirs, subdirs, files in os.walk(testdata_path):\n",
    "    numbers_of_testdata += len(files)\n",
    "\n",
    "testfileName = [file for file in listdir(testdata_path) if isfile(join(testdata_path, file))]\n",
    "\n",
    "for num, fname in enumerate(testfileName):\n",
    "    path = testdata_path + r\"\\\\\" + fname\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)        \n",
    "    test_data.append(img)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "print(test_data.shape)\n",
    "\n",
    "########################################################################################\n",
    "test_data = test_data.astype('float32')\n",
    "test_data /= 255\n",
    "########################################################################################\n",
    "prediction = model.predict(test_data)\n",
    "filecounter = 0\n",
    "with open(\"410785018.txt\", 'w') as f:\n",
    "    for predict in prediction:\n",
    "        path = testdata_path + r'\\\\' + testfileName[filecounter]\n",
    "        txt_name = os.path.splitext(testfileName[filecounter].split('.')[0])[0]\n",
    "        f.write(txt_name + \" \" + (np.argmax(predict)).astype(str) + \"\\n\")\n",
    "        filecounter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8d6b53",
   "metadata": {},
   "source": [
    "# PLT(Full Processing)(Batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aca456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "val_acc=history.history['val_accuracy']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(30)\n",
    "\n",
    "plt.plot(epochs,acc,'r-',label='Training Accuracy')\n",
    "plt.plot(epochs,val_acc,'b-',label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs,loss,'r-',label='Training Loss')\n",
    "plt.plot(epochs,val_loss,'b-',label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bcb781",
   "metadata": {},
   "source": [
    "# PLT(Full Processing)(Batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "val_acc=history.history['val_accuracy']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(50)\n",
    "\n",
    "plt.plot(epochs,acc,'r-',label='Training Accuracy')\n",
    "plt.plot(epochs,val_acc,'b-',label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs,loss,'r-',label='Training Loss')\n",
    "plt.plot(epochs,val_loss,'b-',label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d634c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9e33b020143d376ec311d13df8a0a1ec775ac79d94ca67306768e6dfa3e2290"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
